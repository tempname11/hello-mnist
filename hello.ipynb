{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# SETTINGS\n",
    "learning_rate = 1e-2\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "\tdevice = 'cuda'\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "mnist_train = torchvision.datasets.MNIST('.', train=True, download=True)\n",
    "mnist_test = torchvision.datasets.MNIST('.', train=False, download=True)\n",
    "\n",
    "data_train = mnist_train.data.to(device).float()\n",
    "data_test = mnist_test.data.to(device).float()\n",
    "\n",
    "target_train = mnist_train.targets.to(device)\n",
    "target_test = mnist_test.targets.to(device)\n",
    "\n",
    "print(data_train.shape, data_test.shape)\n",
    "print(data_train.dtype, data_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.net = nn.Sequential(\n",
    "\t\t\tnn.Linear(784, 800),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.LayerNorm(800),\n",
    "\t\t\tnn.Dropout(0.2),\n",
    "\t\t\tnn.Linear(800, 10),\n",
    "\t\t)\n",
    "\t\t\n",
    "\tdef forward(self, x, y=None):\n",
    "\t\tr = x.view(-1, 784)\n",
    "\t\tr = self.net(r)\n",
    "\t\tif y is None:\n",
    "\t\t\tloss = None\n",
    "\t\telse:\n",
    "\t\t\tloss = F.cross_entropy(r, y)\n",
    "\t\treturn r, loss\n",
    "\n",
    "model = Model().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_iters = 4\n",
    "estimation_batch = 256\n",
    "\n",
    "def get_split(split):\n",
    "\tdata = data_train if split == 'train' else data_test\n",
    "\ttarget = target_train if split == 'train' else target_test\n",
    "\treturn data, target\n",
    "\n",
    "def get_batch(batch_size, split):\n",
    "\tdata, target = get_split(split)\n",
    "\tix = torch.randint(0, len(data), (batch_size,))\n",
    "\tx = torch.stack([data[i] for i in ix])\n",
    "\ty = torch.stack([target[i] for i in ix])\n",
    "\treturn x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(split = None):\n",
    "\tif split == None:\n",
    "\t\treturn estimate_loss('train'), estimate_loss('test')\n",
    "\tlosses = torch.zeros(estimation_iters)\n",
    "\tfor i in range(estimation_iters):\n",
    "\t\tx, y = get_batch(estimation_batch, split)\n",
    "\t\tlogits, loss = model.forward(x, y)\n",
    "\t\tlosses[i] = loss\n",
    "\treturn losses.mean().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def total_loss(split = None):\n",
    "\ttotal_estimation_batch = 1 # ?\n",
    "\tif split == None:\n",
    "\t\treturn total_loss('train'), total_loss('test')\n",
    "\tdata, target = get_split(split)\n",
    "\tlosses = torch.zeros(len(data) // total_estimation_batch)\n",
    "\tfor i in range(len(data) // total_estimation_batch):\n",
    "\t\tlower = i * total_estimation_batch\n",
    "\t\tupper = min((i + 1) * total_estimation_batch, len(data))\n",
    "\t\tx = data[lower:upper]\n",
    "\t\ty = target[lower:upper]\n",
    "\t\tlogits, loss = model.forward(x, y)\n",
    "\t\tlosses[i] = loss\n",
    "\treturn losses.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: loss_train=2.4548287391662598, loss_test=2.396651029586792\n",
      "iter 10: loss_train=1.689379096031189, loss_test=1.7093123197555542\n",
      "iter 20: loss_train=0.5847361087799072, loss_test=0.6152201890945435\n",
      "iter 30: loss_train=0.6434414386749268, loss_test=0.6174726486206055\n",
      "iter 40: loss_train=0.5337696671485901, loss_test=0.5469639897346497\n",
      "iter 50: loss_train=0.38543641567230225, loss_test=0.387143611907959\n",
      "iter 60: loss_train=0.34983181953430176, loss_test=0.4302836060523987\n",
      "iter 70: loss_train=0.3249760568141937, loss_test=0.31451690196990967\n",
      "iter 80: loss_train=0.3804546594619751, loss_test=0.3252601623535156\n",
      "iter 90: loss_train=0.3583514988422394, loss_test=0.3323138356208801\n",
      "iter 100: loss_train=0.3735160827636719, loss_test=0.3356773257255554\n",
      "iter 110: loss_train=0.26930326223373413, loss_test=0.311576783657074\n",
      "iter 120: loss_train=0.29076698422431946, loss_test=0.2708670198917389\n",
      "iter 130: loss_train=0.31423336267471313, loss_test=0.3286907374858856\n",
      "iter 140: loss_train=0.2866102457046509, loss_test=0.2866339087486267\n",
      "iter 150: loss_train=0.4155441224575043, loss_test=0.3109622597694397\n",
      "iter 160: loss_train=0.25790369510650635, loss_test=0.29245346784591675\n",
      "iter 170: loss_train=0.2624415159225464, loss_test=0.24095070362091064\n",
      "iter 180: loss_train=0.24039310216903687, loss_test=0.2563304603099823\n",
      "iter 190: loss_train=0.22985008358955383, loss_test=0.2404065877199173\n",
      "iter 200: loss_train=0.23913344740867615, loss_test=0.2614136338233948\n",
      "iter 210: loss_train=0.24657997488975525, loss_test=0.24086810648441315\n",
      "iter 220: loss_train=0.23510614037513733, loss_test=0.19669540226459503\n",
      "iter 230: loss_train=0.24013125896453857, loss_test=0.22799374163150787\n",
      "iter 240: loss_train=0.2494090497493744, loss_test=0.2513374090194702\n",
      "iter 250: loss_train=0.19206593930721283, loss_test=0.21777604520320892\n",
      "iter 260: loss_train=0.22806335985660553, loss_test=0.21250976622104645\n",
      "iter 270: loss_train=0.21161752939224243, loss_test=0.22147230803966522\n",
      "iter 280: loss_train=0.33747240900993347, loss_test=0.26198095083236694\n",
      "iter 290: loss_train=0.27333563566207886, loss_test=0.19759485125541687\n",
      "iter 300: loss_train=0.2534826397895813, loss_test=0.18027591705322266\n",
      "iter 310: loss_train=0.22983179986476898, loss_test=0.23332934081554413\n",
      "iter 320: loss_train=0.20949751138687134, loss_test=0.22314214706420898\n",
      "iter 330: loss_train=0.15948855876922607, loss_test=0.19722725450992584\n",
      "iter 340: loss_train=0.1840682029724121, loss_test=0.18874743580818176\n",
      "iter 350: loss_train=0.147599458694458, loss_test=0.2298446148633957\n",
      "iter 360: loss_train=0.1924380362033844, loss_test=0.2118111550807953\n",
      "iter 370: loss_train=0.19466619193553925, loss_test=0.20148524641990662\n",
      "iter 380: loss_train=0.19640453159809113, loss_test=0.2429962158203125\n",
      "iter 390: loss_train=0.1748739629983902, loss_test=0.18763399124145508\n",
      "iter 400: loss_train=0.17729318141937256, loss_test=0.1678638458251953\n",
      "iter 410: loss_train=0.1542389839887619, loss_test=0.16378284990787506\n",
      "iter 420: loss_train=0.19322830438613892, loss_test=0.15990100800991058\n",
      "iter 430: loss_train=0.1383860856294632, loss_test=0.12053225934505463\n",
      "iter 440: loss_train=0.2384996861219406, loss_test=0.22332040965557098\n",
      "iter 450: loss_train=0.17427504062652588, loss_test=0.2449033260345459\n",
      "iter 460: loss_train=0.24954065680503845, loss_test=0.20102739334106445\n",
      "iter 470: loss_train=0.2350909262895584, loss_test=0.20359820127487183\n",
      "iter 480: loss_train=0.147348552942276, loss_test=0.1513046771287918\n",
      "iter 490: loss_train=0.17568889260292053, loss_test=0.17075905203819275\n",
      "iter 500: loss_train=0.17204676568508148, loss_test=0.16381406784057617\n",
      "iter 510: loss_train=0.14883114397525787, loss_test=0.1960372030735016\n",
      "iter 520: loss_train=0.10286617279052734, loss_test=0.17876802384853363\n",
      "iter 530: loss_train=0.16090214252471924, loss_test=0.18573588132858276\n",
      "iter 540: loss_train=0.16989746689796448, loss_test=0.1847103089094162\n",
      "iter 550: loss_train=0.1500629186630249, loss_test=0.18137551844120026\n",
      "iter 560: loss_train=0.1635781228542328, loss_test=0.1991378515958786\n",
      "iter 570: loss_train=0.1409011334180832, loss_test=0.13340313732624054\n",
      "iter 580: loss_train=0.16080500185489655, loss_test=0.16722002625465393\n",
      "iter 590: loss_train=0.18536004424095154, loss_test=0.19374170899391174\n",
      "iter 600: loss_train=0.11348684132099152, loss_test=0.15107986330986023\n",
      "iter 610: loss_train=0.10864000022411346, loss_test=0.17521271109580994\n",
      "iter 620: loss_train=0.13434302806854248, loss_test=0.13237352669239044\n",
      "iter 630: loss_train=0.15308037400245667, loss_test=0.15476897358894348\n",
      "iter 640: loss_train=0.17098043859004974, loss_test=0.09724760800600052\n",
      "iter 650: loss_train=0.1089959666132927, loss_test=0.14869028329849243\n",
      "iter 660: loss_train=0.14020277559757233, loss_test=0.163253515958786\n",
      "iter 670: loss_train=0.13640552759170532, loss_test=0.1696700006723404\n",
      "iter 680: loss_train=0.09019701927900314, loss_test=0.14035317301750183\n",
      "iter 690: loss_train=0.1627199798822403, loss_test=0.1625625193119049\n",
      "iter 700: loss_train=0.15089131891727448, loss_test=0.17478099465370178\n",
      "iter 710: loss_train=0.15450944006443024, loss_test=0.15374962985515594\n",
      "iter 720: loss_train=0.19231164455413818, loss_test=0.19393351674079895\n",
      "iter 730: loss_train=0.1577337384223938, loss_test=0.20484846830368042\n",
      "iter 740: loss_train=0.13478507101535797, loss_test=0.12194303423166275\n",
      "iter 750: loss_train=0.14288000762462616, loss_test=0.1363927125930786\n",
      "iter 760: loss_train=0.12256278097629547, loss_test=0.12795712053775787\n",
      "iter 770: loss_train=0.17011597752571106, loss_test=0.21493659913539886\n",
      "iter 780: loss_train=0.1660878211259842, loss_test=0.15391649305820465\n",
      "iter 790: loss_train=0.1668260395526886, loss_test=0.1929434984922409\n",
      "iter 800: loss_train=0.1644769161939621, loss_test=0.18119078874588013\n",
      "iter 810: loss_train=0.1890992522239685, loss_test=0.19424627721309662\n",
      "iter 820: loss_train=0.16915950179100037, loss_test=0.17170849442481995\n",
      "iter 830: loss_train=0.14749561250209808, loss_test=0.18442010879516602\n",
      "iter 840: loss_train=0.17061357200145721, loss_test=0.1548394411802292\n",
      "iter 850: loss_train=0.12391948699951172, loss_test=0.12872664630413055\n",
      "iter 860: loss_train=0.15694305300712585, loss_test=0.12669610977172852\n",
      "iter 870: loss_train=0.15337321162223816, loss_test=0.18103086948394775\n",
      "iter 880: loss_train=0.0974823608994484, loss_test=0.1467076539993286\n",
      "iter 890: loss_train=0.14606481790542603, loss_test=0.145658940076828\n",
      "iter 900: loss_train=0.10559196770191193, loss_test=0.11127562820911407\n",
      "iter 910: loss_train=0.13603177666664124, loss_test=0.11167988926172256\n",
      "iter 920: loss_train=0.1584700047969818, loss_test=0.1494811624288559\n",
      "iter 930: loss_train=0.1628856360912323, loss_test=0.12601244449615479\n",
      "iter 940: loss_train=0.13031433522701263, loss_test=0.11403258144855499\n",
      "iter 950: loss_train=0.12743189930915833, loss_test=0.11711014807224274\n",
      "iter 960: loss_train=0.06672915816307068, loss_test=0.09663337469100952\n",
      "iter 970: loss_train=0.14715225994586945, loss_test=0.13564056158065796\n",
      "iter 980: loss_train=0.10182041674852371, loss_test=0.15572024881839752\n",
      "iter 990: loss_train=0.117306649684906, loss_test=0.10549430549144745\n"
     ]
    }
   ],
   "source": [
    "training_iters = 1000\n",
    "training_batch = 64\n",
    "eval_interval = 10\n",
    "batch_losses = []\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for iter in range(training_iters):\n",
    "\tif (iter % eval_interval) == 0:\n",
    "\t\tloss_train, loss_test = estimate_loss()\n",
    "\t\ttest_losses.append(loss_test)\n",
    "\t\ttrain_losses.append(loss_train)\n",
    "\t\tprint(f'iter {iter}: loss_train={loss_train}, loss_test={loss_test}')\n",
    "\n",
    "\tx, y = get_batch(training_batch, 'train')\n",
    "\tlogits, loss = model(x, y)\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\tbatch_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_max = 0\n",
    "accuracy_mno = 0\n",
    "for i in range(len(data_test)):\n",
    "\tlogits, _ = model(data_test[i])\n",
    "\tresult_max = logits.argmax()\n",
    "\tresult_mno = torch.multinomial(F.softmax(logits, dim=-1), 1)\n",
    "\tif result_max == target_test[i]:\n",
    "\t\taccuracy_max += 1\n",
    "\tif result_mno == target_test[i]:\n",
    "\t\taccuracy_mno += 1\n",
    "\n",
    "accuracy_max /= len(data_test)\n",
    "accuracy_mno /= len(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = total_loss('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9517 0.9378 0.15387371182441711 0.8573802911674455\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(accuracy_max, accuracy_mno, loss, math.exp(-loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 0.000\n",
      "1: 0.000\n",
      "2: 0.000\n",
      "3: 0.001\n",
      "4: 0.000\n",
      "5: 0.993\n",
      "6: 0.000\n",
      "7: 0.000\n",
      "8: 0.007\n",
      "9: 0.000\n",
      "max=5, target: 5\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA0UlEQVR4nGNgGMTA//B8D1xyYl///fvSjyzCgmDmcDIwcCUzn/q4GYvOm//+/fv3/9+/f79//96QhaaTgYFhzrl9DLLON9mF1t5G05n97J82btdu/2ePKsCExL7C4INbp/SPc7glGWb9jcZpLMM3xjBGnDoFH/0LwG1u9b/znDglFW7/i8EmXsvMwMDAEPzvAzbJQFcGBgYGjic/rLBITvnopsXCwOD27wIHFtnj//69KWEw/PdPGIske/6hv38evfh3HYd7S+c9+ndeB7scFQEANPVIBKARfBgAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_one(index):\n",
    "\ty, _ = model(data_test[index])\n",
    "\ts = torch.softmax(y, dim=-1)\n",
    "\tm = torch.argmax(y)\n",
    "\tfor v, i in zip(s[0], range(10)):\n",
    "\t\tprint(\"{}: {:.3f}\".format(i, v))\n",
    "\tprint(f'max={m}, target: {target_test[index]}')\n",
    "\treturn mnist_test[index][0]\n",
    "\n",
    "import random\n",
    "predict_one(random.randint(0, data_test.shape[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-notebook-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
